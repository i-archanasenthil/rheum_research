{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio , re , httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional , AsyncIterator , List\n",
    "import urllib.parse\n",
    "import requests\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_variables = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiley_key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 08:47:52.766480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749912472.830036 1844935 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749912472.847407 1844935 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749912472.966017 1844935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749912472.966044 1844935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749912472.966047 1844935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749912472.966049 1844935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-14 08:47:52.981655: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from functions.paper_to_doi import get_info_from_doi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Easy_Publisher_PDF_Resolver:\n",
    "#     \"\"\"\n",
    "#     Resolve a DOI to a direct PDF URL for Springer, OUP, Hindawi, F1000, etc.\n",
    "#     Raises NeedsBrowser when the easy path fails.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     _SPRINGER_HOST     = \"link.springer.com\"\n",
    "#     _OUP_HOST_RE       = re.compile(r\"^https?://academic\\.oup\\.com/\")\n",
    "#     _F1000_HOST_RE     = re.compile(r\"^https?://(?:f1000research|wellcomeopenresearch|gatesopenresearch)\\.org/\")\n",
    "#     _HINDAWI_DOWNLOAD  = re.compile(r\"^https?://downloads\\.hindawi\\.com/\")\n",
    "#     _HINDAWI_LANDING   = re.compile(r\"^https?://(?:www\\.)?hindawi\\.com/\")\n",
    "\n",
    "    \n",
    "#     _ANCHOR_PDF_CLASS_RE = re.compile(r\"pdf|download\", re.I)  # class / id hints\n",
    "\n",
    "\n",
    "#     class NeedsBrowser(RuntimeError):\n",
    "#         \"\"\"Raised when the resolver needs a browser to resolve the URL.\"\"\"\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "#     def __init__(self,*,\n",
    "#                  headers: Optional[dict] = None,\n",
    "#                  timeout: int = 30):\n",
    "#         self.headers = headers or {\n",
    "#             \"User-Agent\": \"Mozilla/5.0 (easy-pdf-resolver/0.2)\"\n",
    "#         }\n",
    "#         self.timeout = timeout\n",
    "#         self._client : Optional[httpx.AsyncClient] = None\n",
    "      \n",
    "#     async def __aenter__(self): \n",
    "#         self._client = httpx.AsyncClient(\n",
    "#             headers=self.headers,\n",
    "#             follow_redirects=True, \n",
    "#             timeout=self.timeout\n",
    "#         )\n",
    "        \n",
    "#         return self\n",
    "    \n",
    "#     async def __aexit__(self, exc_type, exc, traceback):\n",
    "#         await self._client.aclose()\n",
    "#         self._client = None\n",
    "        \n",
    "#     def _require_client(self) -> httpx.AsyncClient:\n",
    "#         if self._client is None:\n",
    "#             # allow use without 'async with' by lazily creating a client\n",
    "#             self._client = httpx.AsyncClient(\n",
    "#                 headers=self.headers,\n",
    "#                 follow_redirects=True,\n",
    "#                 timeout=self.timeout\n",
    "#             )\n",
    "#         return self._client\n",
    "\n",
    "    \n",
    "#     async def _is_pdf(self, url: str) -> bool:\n",
    "#         client = self._require_client()\n",
    "#         r = await client.head(url)\n",
    "#         return (\n",
    "#             r.status_code == 200 and\n",
    "#             r.headers.get(\"content-type\", \"\").startswith(\"application/pdf\")\n",
    "#         )\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def _extract_meta_pdf(html: str) -> Optional[str]:\n",
    "#         \"\"\"\n",
    "#         Extract the PDF URL advertised in a page’s ``<meta>`` tag.\n",
    "\n",
    "#         Many scholarly-publisher landing pages include\n",
    "#         ``<meta name=\"citation_pdf_url\" content=\"https://…/article.pdf\">``  \n",
    "#         inside their ``<head>`` section.  This helper pulls that value out\n",
    "#         so the scraper can download the PDF without further heuristics.\n",
    "\n",
    "#         Args:\n",
    "#         html (str):\n",
    "#             Raw HTML source of the article landing page\n",
    "#             (typically obtained via ``await http_client.get(url).text``).\n",
    "\n",
    "#         Returns:\n",
    "#             Optional[str]:\n",
    "#                 • The PDF URL found in the ``content`` attribute of the first\n",
    "#                 ``<meta name=\"citation_pdf_url\">`` tag, **or**  \n",
    "#                 • ``None`` if no such tag exists (caller should try another\n",
    "#                 strategy, e.g. Playwright).\n",
    "\n",
    "#         Example:\n",
    "#             >>> landing_html = client.get(\"https://academic.oup.com/jn/article/...\", timeout=30).text\n",
    "#             >>> pdf_url = _extract_meta_pdf(landing_html)\n",
    "#             >>> print(pdf_url)\n",
    "#             https://academic.oup.com/jn/article-pdf/123/4/1234/9876543/jn.123.4.1234.pdf\n",
    "\n",
    "#         Notes:\n",
    "#             * Uses BeautifulSoup’s default parser (``html.parser``) to avoid\n",
    "#             external dependencies.  Switch to ``\"lxml\"`` if you already have\n",
    "#             lxml installed and want a performance boost.\n",
    "#             * Only the **first** matching tag is returned; this is sufficient\n",
    "#             for OUP, Wiley HTML, and most CrossRef-compliant publishers.\n",
    "\n",
    "#         \"\"\"\n",
    "#         soup = BeautifulSoup(html, \"html.parser\")\n",
    "#         tag  = soup.find(\"meta\", attrs={\"name\": \"citation_pdf_url\"})\n",
    "#         return tag[\"content\"] if tag else None\n",
    "    \n",
    "    \n",
    "#     @classmethod\n",
    "#     def _extract_anchor_pdf(cls, html: str, base_url: str) -> Optional[str]:\n",
    "#         \"\"\"Return absolute URL from the *best* `<a>` that seems to be a PDF link.\n",
    "\n",
    "#         Heuristics (checked in order):\n",
    "#         1. `href` ends with `.pdf` (common for Springer / OUP).\n",
    "#         2. `href` contains `/pdf` *and* anchor text or class‑attribute mentions\n",
    "#            \"pdf\" or \"download\" (F1000 style: `/pdf?article_uuid=…`).\n",
    "#         3. Anchor `class` / `id` matches /pdf|download/ even if `href` is just\n",
    "#            a relative `/content/pdf/…` path.\n",
    "#         \"\"\"\n",
    "#         soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#         def make_abs(href: str) -> str:\n",
    "#             return urllib.parse.urljoin(base_url, href)\n",
    "\n",
    "#         # Pass 0 – explicit OUP pattern /article-pdf/ ---------------------------\n",
    "#         for a in soup.find_all(\"a\", href=True):\n",
    "#             href = a[\"href\"].strip()\n",
    "#             if \"/article-pdf/\" in href.lower():\n",
    "#                 return make_abs(href)\n",
    "\n",
    "#         # Pass 1 – href ends with .pdf -----------------------------------------\n",
    "#         for a in soup.find_all(\"a\", href=True):\n",
    "#             href = a[\"href\"].strip()\n",
    "#             if href.lower().endswith(\".pdf\"):\n",
    "#                 return make_abs(href)\n",
    "\n",
    "#         # Pass 2 – href contains \"pdf\" + textual or class cues -----------------\n",
    "#         for a in soup.find_all(\"a\", href=True):\n",
    "#             href = a[\"href\"].strip()\n",
    "#             href_lower = href.lower()\n",
    "#             if \"pdf\" in href_lower:\n",
    "#                 text = a.get_text(\" \", strip=True).lower()\n",
    "#                 classes = \" \".join(a.get(\"class\", [])).lower()\n",
    "#                 ident   = a.get(\"id\", \"\").lower()\n",
    "#                 if (\"pdf\" in text or \"download\" in text or cls._ANCHOR_HINT_RE.search(classes) or cls._ANCHOR_HINT_RE.search(ident)):\n",
    "#                     return make_abs(href)\n",
    "\n",
    "#         # Pass 3 – only class/id hints -----------------------------------------\n",
    "#         for a in soup.find_all(\"a\", href=True):\n",
    "#             classes = \" \".join(a.get(\"class\", []))\n",
    "#             ident   = a.get(\"id\", \"\")\n",
    "#             if cls._ANCHOR_HINT_RE.search(classes) or cls._ANCHOR_HINT_RE.search(ident):\n",
    "#                 return make_abs(a[\"href\"].strip())\n",
    "\n",
    "#         return None\n",
    "\n",
    "        \n",
    "       \n",
    "        \n",
    "#     async def get_pdf(self, doi: str) -> str:\n",
    "        \n",
    "#         \"\"\"\n",
    "#         Return a direct PDF URL for the given DOI.\n",
    "#         Raises EasyPDFResolver.NeedsBrowser if the easy strategies fail.\n",
    "#         \"\"\"\n",
    "#         client = self._client or httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=self.timeout)\n",
    "#         landing = str((await client.get(f\"https://doi.org/{doi}\")).url)\n",
    "        \n",
    "#         if self._SPRINGER_HOST in landing:\n",
    "#             candidates = [\n",
    "#                 f\"https://{self._SPRINGER_HOST}/content/pdf/{doi}.pdf\",\n",
    "#             ]\n",
    "#             # /fulltext.html → .pdf\n",
    "#             if landing.endswith(\"fulltext.html\") or landing.endswith(\"fulltext.htm\"):\n",
    "#                 candidates.append(landing.rsplit(\"/\", 1)[0] + \".pdf\")\n",
    "#             # article path without extension → add .pdf\n",
    "#             elif landing.endswith(\".html\"):\n",
    "#                 candidates.append(landing[:-5] + \".pdf\")\n",
    "#             else:\n",
    "#                 if \"/article/\" in landing and not landing.endswith(\".pdf\"):\n",
    "#                     candidates.append(landing + \".pdf\")\n",
    "\n",
    "#             for pdf_url in candidates:\n",
    "#                 if await self._is_pdf(pdf_url):\n",
    "#                     return pdf_url\n",
    "\n",
    "#             # 🚀 NEW: scrape the Download‑PDF button\n",
    "\n",
    "#         # ------------------------------------------------------------------\n",
    "#         # Hindawi – DOI redirect or downloads.hindawi.com\n",
    "#         # ------------------------------------------------------------------\n",
    "#         if self._HINDAWI_DOWNLOAD.match(landing) or self._HINDAWI_LANDING.match(landing):\n",
    "#             for cand in {landing, f\"https://doi.org/{doi}\"}:\n",
    "#                 if await self._is_pdf(cand):\n",
    "#                     return cand\n",
    "#             # try landing→html parse as last resort\n",
    "#             html = (await client.get(landing)).text\n",
    "#             meta = self._extract_meta_pdf(html)\n",
    "#             if meta and await self._is_pdf(meta):\n",
    "#                 return meta\n",
    "\n",
    "#         # ------------------------------------------------------------------\n",
    "#         # F1000Research family – JSON API\n",
    "#         # ------------------------------------------------------------------\n",
    "#         if self._F1000_HOST_RE.match(landing):\n",
    "#             article_id = doi.split(\"/\")[-1]\n",
    "#             host = urllib.parse.urlparse(landing).hostname\n",
    "#             api_url = f\"https://api.{host}/article/{article_id}\"\n",
    "#             try:\n",
    "#                 pdf_url = (await client.get(api_url)).json()[\"data\"][\"pdf_url\"]\n",
    "#                 if await self._is_pdf(pdf_url):\n",
    "#                     return pdf_url\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "\n",
    "#         # ------------------------------------------------------------------\n",
    "#         # OUP – meta tag only\n",
    "#         # ------------------------------------------------------------------\n",
    "#         if self._OUP_HOST_RE.match(landing):\n",
    "#             html = (await client.get(landing)).text\n",
    "#             meta_link = self._extract_meta_pdf(html)\n",
    "#             if meta_link and await self._is_pdf(meta_link):\n",
    "#                 return meta_link\n",
    "#         try:\n",
    "#             html = (await client.get(landing)).text\n",
    "#             anchor_link = self._extract_anchor_pdf(html, landing)\n",
    "#             if anchor_link and await self._is_pdf(anchor_link):\n",
    "#                 return anchor_link\n",
    "#         except httpx.HTTPError:\n",
    "#             pass \n",
    "#         # ------------------------------------------------------------------\n",
    "#         # Escalate\n",
    "#         # ------------------------------------------------------------------\n",
    "#         raise self.NeedsBrowser(f\"Browser required for {doi} → {landing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyPDFResolver:\n",
    "    _SPRINGER_HOST = \"link.springer.com\"\n",
    "    _OUP_HOST_RE = re.compile(r\"^https?://academic\\.oup\\.com/\")\n",
    "    _F1000_HOST_RE = re.compile(r\"^https?://(?:f1000research|wellcomeopenresearch|gatesopenresearch)\\.org/\")\n",
    "    _HINDAWI_DOWNLOAD_RE = re.compile(r\"^https?://downloads\\.hindawi\\.com/\")\n",
    "    _HINDAWI_LANDING_RE = re.compile(r\"^https?://(?:www\\.)?hindawi\\.com/\")\n",
    "    _ANCHOR_HINT_RE = re.compile(r\"pdf|download\", re.I)\n",
    "    wiley_token = env_variables.get(\"wiley_api_token\")\n",
    "\n",
    "    class NeedsBrowser(RuntimeError):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, *, headers: Optional[dict] = None, timeout: int = 30):\n",
    "        self.headers = headers or {\"User-Agent\": \"Mozilla/5.0 (easy-pdf-resolver/1.2)\"}\n",
    "        self.timeout = timeout\n",
    "        self._client: Optional[httpx.AsyncClient] = None\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        self._client = httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=self.timeout)\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        if self._client:\n",
    "            await self._client.aclose()\n",
    "            self._client = None\n",
    "\n",
    "    def _client_required(self) -> httpx.AsyncClient:\n",
    "        if self._client is None:\n",
    "            self._client = httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=self.timeout)\n",
    "        return self._client\n",
    "\n",
    "    async def get_pdf(self, doi: str) -> str:\n",
    "        client = self._client_required()\n",
    "        landing = str((await client.get(f\"https://doi.org/{doi}\")).url)\n",
    "\n",
    "        if self._SPRINGER_HOST in landing:\n",
    "            for url in self._springer_candidates(landing, doi):\n",
    "                if await self._is_pdf(url):\n",
    "                    return url\n",
    "\n",
    "        if self._HINDAWI_DOWNLOAD_RE.match(landing) or self._HINDAWI_LANDING_RE.match(landing):\n",
    "            for url in (landing, f\"https://doi.org/{doi}\"):\n",
    "                if await self._is_pdf(url):\n",
    "                    return url\n",
    "\n",
    "        if self._F1000_HOST_RE.match(landing):\n",
    "            api_pdf = await self._f1000_pdf(landing, doi)\n",
    "            if api_pdf and await self._is_pdf(api_pdf):\n",
    "                return api_pdf\n",
    "\n",
    "        if self._OUP_HOST_RE.match(landing):\n",
    "            oup_pdf = await self._oup_pdf(landing)\n",
    "            if oup_pdf:\n",
    "                return oup_pdf\n",
    "\n",
    "        html = (await client.get(landing)).text\n",
    "        anchor = self._extract_anchor_pdf(html, landing)\n",
    "        if anchor:\n",
    "            return anchor\n",
    "\n",
    "        cr_pdf = self._crossref_fallback(doi)\n",
    "        if cr_pdf:\n",
    "            return cr_pdf\n",
    "\n",
    "        raise self.NeedsBrowser(f\"Browser required for {doi} → {landing}\")\n",
    "\n",
    "    def _springer_candidates(self, landing: str, doi: str) -> List[str]:\n",
    "        base = f\"https://{self._SPRINGER_HOST}\"\n",
    "        candidates = [f\"{base}/content/pdf/{doi}.pdf\"]\n",
    "        if landing.endswith((\"fulltext.html\", \"fulltext.htm\")):\n",
    "            candidates.append(landing.rsplit(\"/\", 1)[0] + \".pdf\")\n",
    "        elif landing.endswith(\".html\"):\n",
    "            candidates.append(landing[:-5] + \".pdf\")\n",
    "        elif \"/article/\" in landing and not landing.endswith(\".pdf\"):\n",
    "            candidates.append(landing + \".pdf\")\n",
    "        return candidates\n",
    "\n",
    "    async def _f1000_pdf(self, landing: str, doi: str) -> Optional[str]:\n",
    "        client = self._client_required()\n",
    "        article_id = doi.split(\"/\")[-1]\n",
    "        host = urllib.parse.urlparse(landing).hostname\n",
    "        api_url = f\"https://api.{host}/article/{article_id}\"\n",
    "        try:\n",
    "            r = await client.get(api_url)\n",
    "            r.raise_for_status()\n",
    "            return r.json()[\"data\"][\"pdf_url\"]\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    async def _oup_pdf(self, landing: str) -> Optional[str]:\n",
    "        client = self._client_required()\n",
    "        html = (await client.get(landing)).text\n",
    "        meta = self._extract_meta_pdf(html)\n",
    "        if meta:\n",
    "            return meta\n",
    "        anchor = self._extract_anchor_pdf(html, landing)\n",
    "        return anchor\n",
    "\n",
    "    async def _wiley_pdf(self , landing:str , doi:str) -> Optional[str]:\n",
    "        client = self._client_required()\n",
    "        \n",
    "        #1 TDM Api\n",
    "        api_url = f\"https://api.wiley.com/tdm/v1/articles?{doi}/pdf\"\n",
    "        r = await client.get(\n",
    "            api_url,\n",
    "            headers= {\"Authorization\": f\"Bearer {self.wiley_token}\"})\n",
    "        if r.status_code == 200 and r.headers.get(\"content-type\", \"\").startswith(\"application/pdf\"):\n",
    "            return api_url\n",
    "        \n",
    "        #pdfdirect \n",
    "        \n",
    "        pdf_url = f\"https://onlinelibrary.wiley.com/doi/pdfdirect/{doi}\"\n",
    "        r2 = await client.get(pdf_url)\n",
    "        if r2.status_code == 200 and r2.headers.get(\"content-type\", \"\").startswith(\"application/pdf\"):\n",
    "            return pdf_url\n",
    "        \n",
    "        #htmlparse \n",
    "        \n",
    "        html = (await client.get(landing)).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        btn = soup.find(\"a\", class_=\"pdf-download-link\", href=True)\n",
    "        if btn:\n",
    "            return urllib.parse.urljoin(landing, btn[\"href\"])\n",
    "        return None \n",
    "    \n",
    "    async def _tand_pdf(self , landing:str , doi:str) -> Optional[str]:\n",
    "        client = self._client_required()\n",
    "        \n",
    "        #direct pdf endpoint \n",
    "        \n",
    "        pdf_endpoint = f\"https://www.tandfonline.com/doi/pdf/{doi}\"\n",
    "        resp = await client.get(pdf_endpoint)\n",
    "        if resp.status_code == 200 and resp.headers.get(\"content-type\", \"\").startswith(\"application/pdf\"):\n",
    "            return pdf_endpoint\n",
    "        \n",
    "        #meta tag\n",
    "        html = (await client.get(landing)).text\n",
    "        if (meta:= self._extract_meta_pdf(html)):\n",
    "            return urllib.parse.urljoin(landing, meta)\n",
    "        \n",
    "        if (anchor := self._extract_anchor_pdf(html, landing)):\n",
    "            return anchor\n",
    "        return None\n",
    "        \n",
    "    async def _sage_pdf(self, landing: str, doi: str) -> Optional[str]:\n",
    "        client = self._client_required()\n",
    "        \n",
    "        #pdf endpoint\n",
    "        \n",
    "        pdf_url = f\"https://journals.sagepub.com/doi/pdf/{doi}\"\n",
    "        r = await client.get(pdf_url)\n",
    "        if r.status_code == 200 and r.headers.get(\"content-type\", \"\").startswith(\"application/pdf\"):\n",
    "            return pdf_url\n",
    "        \n",
    "        html = (await client.get(landing)).text\n",
    "        if (meta:= self._extract_meta_pdf(html)):\n",
    "            return urllib.parse.urljoin(landing, meta)\n",
    "        \n",
    "        if (anchor := self._extract_anchor_pdf(html, landing)):\n",
    "            return anchor\n",
    "        return None\n",
    "        \n",
    "        \n",
    "    async def _karger_pdf(self, landing: str, doi: str) -> Optional[str]:\n",
    "        clinet = self._client_required()\n",
    "        \n",
    "        #must get article id \n",
    "        \n",
    "        html = (await clinet.get(landing)).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        tag = soup.find(\"meta\", attrs={\"name\": \"dc.identifier\"})\n",
    "        if not tag or not tag.get(\"data-article-id\"):\n",
    "            return None\n",
    "        article_id = tag[\"data-article-id\"]\n",
    "        return f\"https://www.karger.com/Article/Pdf/{article_id}\"\n",
    "       \n",
    "    async def _is_pdf(self, url: str) -> bool:\n",
    "        client = self._client_required()\n",
    "        try:\n",
    "            r = await client.head(url)\n",
    "            if r.status_code in (403, 405):\n",
    "                r = await client.get(url, headers={\"Range\": \"bytes=0-0\"})\n",
    "            return r.status_code == 200 and r.headers.get(\"content-type\", \"\").startswith(\"application/pdf\")\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_meta_pdf(html: str) -> Optional[str]:\n",
    "        tag = BeautifulSoup(html, \"html.parser\").find(\"meta\", attrs={\"name\": \"citation_pdf_url\"})\n",
    "        return tag[\"content\"].strip() if tag and tag.get(\"content\") else None\n",
    "\n",
    "    def _extract_anchor_pdf(self, html: str, base_url: str) -> Optional[str]:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        join = lambda h: urllib.parse.urljoin(base_url, h)\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"].strip()\n",
    "            lower = href.lower()\n",
    "            full_url = join(href)\n",
    "            if lower.endswith(\".pdf\") or \"/article-pdf/\" in lower or \"/advance-article-pdf/\" in lower:\n",
    "                return full_url\n",
    "            text = a.get_text(\" \").lower()\n",
    "            cls = \" \".join(a.get(\"class\", [])).lower()\n",
    "            ident = a.get(\"id\", \"\").lower()\n",
    "            if \"download pdf\" in text or self._ANCHOR_HINT_RE.search(cls) or self._ANCHOR_HINT_RE.search(ident):\n",
    "                return full_url\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _crossref_fallback(doi: str) -> Optional[str]:\n",
    "        try:\n",
    "            r = requests.get(f\"https://api.crossref.org/works/{doi}\", timeout=15)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "        except Exception:\n",
    "            return None\n",
    "        links = data.get(\"message\", {}).get(\"link\", [])\n",
    "        best = None\n",
    "        for link in links:\n",
    "            ct = link.get(\"content-type\")\n",
    "            url = link.get(\"URL\")\n",
    "            if ct == \"text/html\":\n",
    "                return url\n",
    "            if ct == \"application/pdf\" and best is None:\n",
    "                best = url\n",
    "            if url and url.endswith(\".pdf\") and best is None:\n",
    "                best = url\n",
    "        return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dois = [\n",
    "    \"10.1155/2022/2836128\" # Hindawi OA\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'http://downloads.hindawi.com/journals/jir/2022/2836128.pdf'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "async with EasyPDFResolver() as r:\n",
    "    pdf_url = await r.get_pdf(test_dois[0])\n",
    "    pprint(pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_info = get_info_from_doi(\"10.1093/rheumatology/keab153\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '10.1093/rheumatology/keab153',\n",
       " 'title': 'Rituximab plus leflunomide in rheumatoid arthritis: a randomized, placebo-controlled, investigator-initiated clinical trial (AMARA study)',\n",
       " 'document_link': 'http://academic.oup.com/rheumatology/advance-article-pdf/doi/10.1093/rheumatology/keab153/36643438/keab153.pdf'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robust_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
